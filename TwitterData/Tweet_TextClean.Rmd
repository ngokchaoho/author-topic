---
title: "TweetDataClean"
author: "CY"
date: "23/02/2019"
output: html_document
---

#Load Packages
```{r}
library(stringr)
library(tm) # Text Mining Package for Tokenization
library(textclean)

```

```{r}
# library(dplyr)
# library(tidyverse) #keepin' things tidy
# library(tidytext) #package for tidy text analysis (Check out Julia Silge's fab book!)
# library(glue) #for pasting strings
# library(data.table) #for rbindlist, a faster version of rbind
# library(lubridate)
# library(textstem)  #to lemmatize
# library(wordcloud2)
# 
# library(text2vec)
# library(glmnet)
# library(pROC)
```


# Read in Files
```{r}
data_an <- read.csv("anondran_tweets.csv", header = T, stringsAsFactors = F, encoding = "UTF-8")
data_dt <- read.csv("dtapscott_tweets.csv", header = T, stringsAsFactors = F, encoding = "UTF-8")
data_fran <- read.csv("francispouliot__tweets.csv", header = T, stringsAsFactors = F, encoding = "UTF-8")
data_Hey <- read.csv("HeyTaiZen_tweets.csv", header = T, stringsAsFactors = F, encoding = "UTF-8")
data_iam <- read.csv("iamjosephyoung_tweets.csv", header = T, stringsAsFactors = F, encoding = "UTF-8")
data_jon <- read.csv("jonmatonis_tweets.csv", header = T, stringsAsFactors = F, encoding = "UTF-8")
data_Tuu <- read.csv("TuurDemeester_tweets.csv", header = T, stringsAsFactors = F, encoding = "UTF-8")
data_Vin <- read.csv("VinnyLingham_tweets.csv", header = T, stringsAsFactors = F, encoding = "UTF-8")
data_woo <- read.csv("woonomic_tweets.csv", header = T, stringsAsFactors = F, encoding = "UTF-8")

authorName <- c("anondran", "dtapscott", "francispouliot", "HeyTaiZen", "iamjosephyoung", "jonmatonis", "TuurDemeester", "VinnyLingham", "woonomic")
authorNameShort <- c("an", "dt", "fran", "Hey", "iam", "jon", "Tuu", "Vin", "woo")

```

# Clean Text
```{r}

# cleanText <- function(t) {
#   t.tmp <- tm_map(t, removePunctuation)
#   t.tmp <- tm_map(t.tmp, stripWhitespace)
#   t.tmp <- tm_map(t.tmp, tolower)
#   t.tmp <- tm_map(t.tmp, removeWords, stopwords("english"))
#   return(t.tmp) 
# }

# keep no
keepNONOT <- function(x) gsub("no |not ", "no_", x)

removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
removeEmoji <- function(x) replace_emoji(x) %>% str_replace_all("\\<.+\\>", "") 
removeSomePunc <- function(x) gsub("[][!()*,;<=>^|~{}]-", "", x)  # keep %,$,@,#,_,                                          

removeExtraPunc <- function(x) {
  x <- gsub("\\,|\\!|\\*|\\:", "", x)  # :) x)
  x <- gsub("'", "", x)
  x <- gsub('"', "", x)
  if (!str_detect(x, "[0-9]+\\.[0-9]+")){
     x <- gsub("\\.", "", x) 
  } 
  else {
    x <- gsub("\\. |\\.^", "", x)
  }
  x <- gsub("\\?+", "\\?", x) 
  return(x)
}
# for loop to deal with all authors
num_author <- length(authorName)
for (num in 1:num_author){
  data <- get(paste0("data", "_",authorNameShort[num])) 
 data$text_clean <- data$text
 data$text_clean <- data$text_clean %>% tolower()
 data$text_clean <- data$text_clean %>% removeURL()
 # Keep No word
  # data$text_clean <- data$text_clean %>% keepNONOT()
 data$text_clean <- data$text_clean %>% removeWords( stopwords("en"))
 # Remove Emoji
 data$text_clean <- data$text_clean %>% removeEmoji()
 data$text_clean <- data$text_clean %>%  removeSomePunc()
  for (i in 1:nrow(data)){
    data$text_clean[i] <- data$text_clean[i] %>%  removeExtraPunc()  
  }
 data$text_clean <- data$text_clean %>%  stripWhitespace()
 data$text_clean <- data$text_clean %>% str_trim()

  for (i in 1:nrow(data)){
    data$final[i] <- str_c(authorName[num], str_replace_all(data$text_clean[i], " ", ":"), sep = "\t")   
  } 
  assign(paste0("data", "_",authorNameShort[num]), data)
  #filename_temp <- paste0(authorName[num], "_keepEmojiNo.tsv")
  #filename_temp <- paste0(authorName[num], "_keepEmoji.tsv")
  #filename_temp <- paste0(authorName[num], "_keepNo.tsv")
  filename_temp <- paste0(authorName[num], "_clean.tsv")
  write(data$final, filename_temp)
}

```

